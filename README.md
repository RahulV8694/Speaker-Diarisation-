# ğŸ™ï¸ Speaker Diarization with PyAnnote

This is a simple speaker diarization pipeline built using [`pyannote-audio`](https://github.com/pyannote/pyannote-audio). It processes a `.mp4` video file to figure out **who spoke when**, leveraging Hugging Face-hosted pretrained models.

---

##  Folder Layout

```
speaker_diarization_project/
â”‚
â”œâ”€â”€ main.py                    # Entry point â€” runs the full pipeline
â”œâ”€â”€ requirements.txt           # List of required Python packages
â”œâ”€â”€ README.md                  # Project documentation (this file)
â”‚
â”œâ”€â”€ utils/                     # Helper modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio_extractor.py     # Converts MP4 to mono WAV (16kHz)
â”‚   â””â”€â”€ diarizer.py            # Runs the diarization and saves plots
â”‚
â”œâ”€â”€ data/                      # Place your input .mp4 file here
â”‚   â””â”€â”€ input_video.mp4
â”‚
â””â”€â”€ outputs/                   # Outputs (audio + speaker timeline plot)
    â”œâ”€â”€ audio.wav
    â””â”€â”€ diarization_plot.png
```

---

##  Getting Started

### 1. Set up your local environment

After cloning or unzipping this folder, navigate to it:

```bash
cd speaker_diarization_project
```

Optionally, set up a virtual environment:

```bash
python -m venv venv
source venv/bin/activate        # For Windows: venv\Scripts\activate
```

Then install dependencies:

```bash
pip install -r requirements.txt
```

---

##  Hugging Face Access

The models used in this project are gated. Youâ€™ll need a Hugging Face token and must manually agree to the model access terms.

### Here's what you need to do:

1. Get a Hugging Face token:  
   [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)  
   Create a new one with **read** access.

2. Accept model access:  
   - [`pyannote/speaker-diarization`](https://huggingface.co/pyannote/speaker-diarization)  
   - [`pyannote/segmentation`](https://huggingface.co/pyannote/segmentation)

3. Once done, paste your token inside `main.py`:

```python
HUGGINGFACE_TOKEN = "your_token_here"
```

---

##  Running the Pipeline

1. Drop your video file into the `data/` folder and rename it to `input_video.mp4`.
2. Then just run:

```bash
python main.py
```

---

##  What Youâ€™ll Get

- Extracted audio (mono, 16kHz) saved as â†’ `outputs/audio.wav`
- Speaker diarization timeline plot â†’ `outputs/diarization_plot.png`
- Console output showing timestamps and speaker labels


